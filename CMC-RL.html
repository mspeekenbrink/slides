<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Computational Modelling of Cognition</title>
    <meta charset="utf-8" />
    <meta name="author" content="Maarten Speekenbrink" />
    <script src="libs/header-attrs-2.30/header-attrs.js"></script>
    <link rel="stylesheet" href="xaringan-themer.css" type="text/css" />
    <link rel="stylesheet" href="mycss.css" type="text/css" />
    <link rel="stylesheet" href="width.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

.title[
# Computational Modelling of Cognition
]
.subtitle[
## Reinforcement learning
]
.author[
### Maarten Speekenbrink
]

---










class: center, middle, inverse

# Background

---

## What is reinforcement learning?

A multi-faceted theory of *learning by doing*

* Learning via interacting with the environment
  * *active*, rather than *passive* learning
  * sequential; the outcome of future interactions may depend on earlier actions
* Learning involves taking actions and observing their consequences
  * Consequences are changes to the environment and reward (or punishment)
  * Overarching goal is to maximise accumulated rewards

---

## Classical (Pavolovian) conditioning

.pull-left[

&lt;img src="https://stories.kuleuven.be/files/_2400x1492_crop_center-center_61_line/pavlov_c-Science-History-Images-Alamy-web.jpg" width="100%" style="display: block; margin: auto;" /&gt;

]

.pull-right[

An unconditioned stimulus (US, e.g. food) elicits an unconditioned response (UR, e.g. salivating). If the US is preceded by a conditioned stimulus (CS, e.g. tone), a CS-US association `\(V\)` is formed, allowing `\(CS\)` to elicit 
a conditioned response CR

]

--

Rescorla and Wagner (1972) model:

`$$w_{t+1}^x = w_t^x + \alpha_x \beta (\lambda - \sum_{x' \in \text{CS}} w_t^{x'})$$`
* `\(w^x\)` is the associative strength of CS `\(x\)`
* `\(\alpha_x\)` is the *salience* of CS `\(x\)`
* `\(\beta\)` is the *association value* of the US
* `\(\lambda\)` is the maximum association strength of the US

---

## Rescorla-Wagner model

`$$w_{t+1}^x = w_t^x + 0.05 (1 - \sum_{x' \in \text{CS}} w_t^{x'})$$`

&lt;img src="CMC-RL_files/figure-html/unnamed-chunk-2-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

## Rescorla-Wagner model (blocking)

`$$w_{t+1}^x = w_t^x + 0.05 (1 - \sum_{x' \in \text{CS}} w_t^{x'})$$`

&lt;img src="CMC-RL_files/figure-html/unnamed-chunk-3-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

## Rescorla-Wagner model (partial reinforcement)

`$$w_{t+1}^x = w_t^x + 0.05 (1 - \sum_{x' \in \text{CS}} w_t^{x'})$$`

&lt;img src="CMC-RL_files/figure-html/unnamed-chunk-4-1.png" width="80%" style="display: block; margin: auto;" /&gt;

---

&lt;!-- 
## Variations

Bush and Mosteller (`\(w_t^x = p(\text{CR}_t|x)\)`):

`$$w_{t+1}^x = w_t^x + \alpha_x (\lambda - w_t^x)$$`
Rescorla-Wagner:
`$$w_{t+1}^x = w_t^x + \alpha_x \beta (\lambda - w_t^\text{tot})$$`
--&gt;

## Temporal difference learning

The Temporal Difference model (Sutton and Barto, 1990) extends the R-W model to learning in real time and allows prediction of the timing of future USs.

.pull-left[
&lt;img src="img/TD-stimulus-trace.png" width="95%" style="display: block; margin: auto;" /&gt;
]

.pull-right[
&lt;img src="img/TD-stimulus-representations.png" width="70%" style="display: block; margin: auto;" /&gt;
]

&lt;img src="img/TD-US-predictions.png" width="100%" style="display: block; margin: auto;" /&gt;

---

## Reward prediction errors and dopamine

RL models of experience-based learning involve adjustment of values according to prediction errors. Montague, Dayan, and Sejnowski (1996) showed that the phasic activity of dopamine cells in the midbrain reflects prediction errors of rewards.

.pull-left[
&lt;img src="img/RPE-dopamine-firing.png" width="100%" style="display: block; margin: auto;" /&gt;
]

.pull-left[
&lt;img src="img/RPE-dopamine-cells.png" width="60%" style="display: block; margin: auto;" /&gt;
]

---

## Instrumental conditioning

Thorndike (1911) Law of Effect: 

&gt;Of several responses made to the same situation, those which are accompanied...by satisfaction...will be more likely to recur; those which are accompanied...by discomfort  ...will be less likely to occur. The greater the satisfaction or discomfort, the greater the strengthening or weakening of the bond.

--

Skinner (1938) Operant Conditioning

.pull-left[
&lt;img src="img/skinner box.webp" width="100%" style="display: block; margin: auto;" /&gt;
]

.pull-right[
* Reinforcement (positive and negative) strengthens behaviour
* Punishment weakens behaviour
]

&lt;!-- r mcitet("tolman1948cognitive"): Cognitive maps --&gt;

---

## Classical vs instrumental conditioning

In classical conditioning, the unconditioned or conditioned response is involuntary. Instrumental conditioning involves eliciting voluntary actions. Needs to extend prior models with goal-directed *action selection*.

---

class: center, middle, inverse

# Markov Decision Processes

---

## Markov Decision Processes

&lt;img src="https://static.wixstatic.com/media/be7152_c68c151c0c6d4911907458740125e09d~mv2.png/v1/fill/w_740,h_286,al_c,lg_1,q_85,enc_auto/be7152_c68c151c0c6d4911907458740125e09d~mv2.png" width="60%" style="display: block; margin: auto;" /&gt;

A Markov Decision Process (MPD) is a mathematical framework for solving (some) RL problems. An MDP consists of:

- A state space `\(\mathcal{S} = \{s_1, s_2, \ldots, s_n\}\)`
- An action space `\(\mathcal{A} = \{a_1, a_2, \ldots, a_m\}\)`
- A state-action-state-reward transition function `\(p(S_{t+1}, R_{t+1}|S_t, A_t)\)`

The objective is to find a **policy** `\(\pi(s) = p(A_t|S_t)\)` that maximises the expected cumulative (discounted) reward

`$$\mathbb{E}_\pi[\sum_{k=0}^\infty \gamma^k R_{t+k+1}|S_t]$$`

---

## Example: Grid World

.pull-left[
&lt;img src="CMC-RL_files/figure-html/unnamed-chunk-12-1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

.pull-right[

* States `\((\mathcal{S})\)`: 23 available positions (squares) on the grid
* Actions `\((\mathcal{A})\)`: Up, Right, Down, Left

The start state is the top left position. Positions with a "T" transport back to the starting state, regardless of action 
]

--

What is the optimal path?

---

## Prediction: The value function

The value function `\(v_\pi(s)\)` is the expected (discounted) cumulative reward of following a policy `\(\pi(a|s)\)` starting from state `\(s\)`:

`$$v_\pi(s) = \mathbb{E}_\pi[\sum_{k=0}^\infty \gamma^k R_{t+k+1}|S_t = s]$$`
(when in state `\(s\)` at time `\(t\)`, what is the expected cumulative discounted reward `\(R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \ldots\)` the agent will get by following policy `\(\pi\)`?)

--

The value function respects the *Bellman equation*:

`$$v_\pi(s) = \sum_{a} \pi(A_t = a|S_t = s)\sum_{s', r} p(S_{t+1} = s', R_{t+1} = r|S_t = s, A_t = a) \left(r + \gamma v_\pi(s')\right)$$`

--

The optimal value function is the maximum expected cumulative reward over all policies `\(\pi(a|s)\)`:

`$$v_*(s) = \max_{\pi} v_\pi(s)$$`
--

If `\(p(S_{t+1} = s', R_{t+1} = r|S_t = s, A_t = a)\)` is known, the optimal policy can be found by *Dynamic Programming*

---

## Dynamic Programming for Grid World

&lt;iframe width="600" height="400" src="https://mspeekenbrink.github.io/reinforcejs/my_gridworld_dp.html" title="Gridworld DP example" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" style="width:100%; height:100%" allowfullscreen&gt;&lt;/iframe&gt;

---

## Prediction from experience: Temporal difference learning

If `\(p(S_{t+1} = s', R_{t+1} = r|S_t = s, A_t = a)\)` is not known, a policy can be evaluated via experience.

In state `\(S_t\)`, agent takes action `\(A_t\)` according to policy `\(\pi\)`, then moves to state `\(S_{t+1}\)` and obtains reward `\(R_{t+1}\)`. Learning now involves the update:

`$$V_\text{new}(S_t) \leftarrow V_\text{old}(S_t) + \alpha_{t} \left(R_{t+1} + \gamma V_\text{old}(S_{t+1}) - V_\text{old}(S_t)\right)$$`
where `\(\alpha_{t} \in (0, 1]\)` is the **learning rate** at time `\(t\)` and
`$$\delta_{t} = R_{t+1} + \gamma V_\text{old}(S_{t+1}) - V_\text{old}(S_t)$$`
a **prediction error**, because of the Bellman equation `\(v(S_{t}) = R_t + \gamma v(S_{t+1})\)`


---

## Control: The action-value function

Closely related to the value function is the action-value function:

`$$q_\pi(s, a) = \mathbb{E}_\pi[\sum_{k=0}^\infty \gamma^k R_{t+k+1}|S_t=s, A_t = a]$$`
(when in state `\(s\)` at time `\(t\)`, what is the expected cumulative discounted reward the agent will get by taking action `\(a\)` and then following policy `\(\pi\)`?)

The value function can be defined from the action-value function as:
`$$v_\pi(s) = \sum_{a} p(A_t = a|S_t = s) q_\pi(s,a)$$`

The action-value function is useful as it allows evaluating particular actions `\(a\)` in states `\(s\)`, rather than focusing on the whole policy.

---

## Control from experience: SARSA and Q-learning

In state `\(S_t\)`, agent takes action `\(A_t\)`, then moves to state `\(S_{t+1}\)` and obtains reward `\(R_{t+1}\)`. Learning involves the update:

`$$Q_\text{new}(S_t,A_t) \leftarrow Q_\text{old}(S_t,A_t) + \alpha_{t} \delta_t$$`
Two popular learning algorithms differ in the definition of the prediction error:

**SARSA**: `\(\delta_t = R_{t+1} + \gamma Q_\text{old}(S_{t+1},A_{t+1}) - Q_\text{old}(S_t,A_t)\)`

**Q-learning**: `\(\delta_t = R_{t+1} + \gamma \max_a Q_\text{old}(S_{t+1},A_{t+1} = a) - Q_\text{old}(S_t,A_t)\)`

--


If
* all possible actions `\(A_t\)` are taken in each possible state `\(S_t\)` an infinite number of times
* the learning rate to reduces to 0 over time `\((\alpha_{\infty} \rightarrow 0)\)`

Then Q-learning is guaranteed to converge on the optimal action-value function `\(q_*(s, a)\)` (Watkins and Dayan, 1992).

Convergence of SARSA additionally requires `\(A_t\)` to be chosen sensibly (over time always choosing `\(A_t = \arg \max_a Q_j(S_{t+1},a)\)`)

---

## Q-learning for Grid World

&lt;iframe width="600" height="400" src="https://mspeekenbrink.github.io/reinforcejs/my_gridworld_td.html" title="Gridworld TD example" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" style="width:100%; height:100%" allowfullscreen&gt;&lt;/iframe&gt;

---

class: center, middle, inverse

# The exploration-exploitation dilemma

---

## Exploration

Learning the optimal policy from experience **requires exploration**, but choosing sub-optimal actions reduces the cumulative reward. This is the **exploration-exploitation dilemma**:

*Should you choose an option which you know you like (exploit), or a more uncertain option such that by learning about it you might improve future decisions (exploration)?*

.pull-left[
&lt;img src="img/bandit.png" width="90%" style="display: block; margin: auto;" /&gt;
]

.pull-right[
Multi-armed bandits are a simple paradigm to study the exploration-exploitation dilemma.
]


---

## Multi-armed bandits

A multi-armed bandit is one of the simplest RL problems. The setup is:

- There are `\(K\)` bandits ("slot machines"), each with an unknown reward function `\(p(R_t|A_t = k)\)`.
- The action `\(A_t\)` is to play one of the bandits
- The objective is to maximise the cumulative reward 
- There is only a single environment state `\(s\)`, so don't need to worry about state transitions

---

## Multi-armed bandits



&lt;img src="CMC-RL_files/figure-html/bandit-plot-1.png" width="60%" style="display: block; margin: auto;" /&gt;

---

## Bayesian RL

Model of the environment:

`$$\begin{aligned}
p(R_t|A_t = j) &amp;= \mathbf{Normal}(\mu_{j,t}, \sigma_j) \\
p(\mu_{j,t+1}|\mu_{j,t}) &amp;= \mathbf{Normal}(\mu_{j,t}, \sigma_{\xi})
\end{aligned}$$`

--

Start with prior distribution: `\(p(\mu_{j,0}) = \mathbf{Normal}(m_{j,0}, v_{j,0})\)`

--

Update prior to posterior via the **Kalman filter**:
`$$\begin{aligned}
m_{j,t} &amp;= m_{j,t-1} + \kappa_{j,t} (R_{t} - m_{j,t-1}) \\
v_{j,t} &amp;= (1-\kappa_{j,t})(v_{j,t-1} + \sigma_{\xi}^2) \\
\kappa_{j,t} &amp;= \begin{cases} \frac{v_{j,t-1} + \sigma^2_{\xi}}{v_{j,t-1} + \sigma^2_{\xi} + \sigma^2_j} &amp;&amp; A_t = j \\ 0 &amp;&amp; A_t \neq j \end{cases}
\end{aligned}$$`

--

&lt;img src="CMC-RL_files/figure-html/bayesian-learning-1.png" width="90%" style="display: block; margin: auto;" /&gt;

---

## Uncertainty and exploration


Uncertainty ignorant exploration strategies:

* epsilon-greedy: with probability `\(\epsilon \in [0,1]\)` explore uniformly, otherwise exploit
* Softmax: Choose options according to `\(\frac{\exp (\theta \times m_{j,t})}{\sum_k \exp(\theta \times m_{k,t})}\)`

Uncertainty-based exploration strategies:

* Upper-confidence bound (UCB): Choose option with highest upper confidence bound `\(m_{j,t} + \beta \times \sqrt{v}_{j,t}\)`
* Thompson sampling: For each option, sample an expected reward from the posterior distribution, and choose option with highest sampled reward

---

## Uncertainty and exploration


&lt;img src="CMC-RL_files/figure-html/exploration-strategies-plot-1.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="img/ownArticles/speekenbrink-ToPIC.png" width="70%" style="display: block; margin: auto auto auto 0;" /&gt;

&lt;img src="CMC-RL_files/figure-html/kalman-UCB-exploration-1.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="img/ownArticles/speekenbrink-ToPIC.png" width="70%" style="display: block; margin: auto auto auto 0;" /&gt;

Large-scale comparison of computational models showing evidence for
* Bayesian learning (Kalman filter; learning rate determined by uncertainty)
* Uncertainty-guided exploration (akin to Thompson sampling) 

---

&lt;img src="img/ownArticles/stojic-fixation.png" width="90%" style="display: block; margin: auto auto auto 0;" /&gt;

&lt;img src="img/stojic-fixation-design.png" width="90%" style="display: block; margin: auto;" /&gt;

--

Computational modelling and eye-tracking show that estimated value and uncertainty influence:

* Visual fixations (people fixate more on high-value and high-uncertainty options)
* Decisions (people choose high-value and high-uncertainty options more often)
* Fixation on reward feedback (people fixate longer on feedback with higher uncertainty)

---

&lt;img src="img/ownArticles/wu-pressure.png" width="70%" style="display: block; margin: auto auto auto 0;" /&gt;

&lt;img src="img/wu-pressure-design.png" width="90%" style="display: block; margin: auto;" /&gt;

--

Less uncertainty-guidance and more habitual choices under time pressure. 

Balancing exploration and exploitation likely requires time-costly cognitive operations.

---

### Contextual multi-armed bandits

&lt;img src="CMC-RL_files/figure-html/bandit-plot-2-1.png" width="50%" style="display: block; margin: auto;" /&gt;

---

### Gaussian processes

&lt;img src="img/ownArticles/schulz-GP-tutorial.png" width="50%" style="display: block; margin: auto auto auto 0;" /&gt;

&lt;img src="img/GP-example.png" width="80%" style="display: block; margin: auto;" /&gt;

---

### Gaussian processes



&lt;img src="img/ownArticles/schulz-GP-tutorial.png" width="50%" style="display: block; margin: auto auto auto 0;" /&gt;

Gaussian Processes unite instance-based and rule-based theories of function learning.

The covariance kernel induces a similarity space over objects/states. This can be goal-dependent.

Kernels may be complex compositions of simpler kernels (Schulz, Tenenbaum, Duvenaud, Speekenbrink, and Gershman, 2017).

---

&lt;img src="img/ownArticles/wu-search.png" width="60%" style="display: block; margin: auto auto auto 0;" /&gt;


&lt;img src="img/Wu-GPUCB-example.png" width="100%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="img/ownArticles/wu-search.png" width="60%" style="display: block; margin: auto auto auto 0;" /&gt;

&lt;img src="img/Wu-exploration-results.png" width="80%" style="display: block; margin: auto;" /&gt;

---


&lt;img src="img/ownArticles/stojic-novelty.png" width="70%" style="display: block; margin: auto auto auto 0;" /&gt;

&lt;img src="img/novelty-task.png" width="70%" style="display: block; margin: auto;" /&gt;

---


&lt;img src="img/ownArticles/stojic-novelty.png" width="70%" style="display: block; margin: auto auto auto 0;" /&gt;

&lt;img src="img/stojic-predictions-A.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="img/ownArticles/stojic-novelty.png" width="70%" style="display: block; margin: auto auto auto 0;" /&gt;

&lt;img src="img/stojic-predictions-B.png" width="90%" style="display: block; margin: auto;" /&gt;

---

&lt;img src="img/ownArticles/stojic-novelty.png" width="70%" style="display: block; margin: auto auto auto 0;" /&gt;

Evidence for
* Functional generalization (more likely to explore novel option if features indicate it is relatively good)
* Uncertainty guidance (more likely to explore a novel option if value is uncertain)

---

class: center, middle, inverse

# Model-free vs model-based RL

---

## Model-free vs model-based RL

Model-free RL (e.g. Q-learning) learns action-values (expected cumulative reward) without explicit reference to state-action-state transitions. Model-free RL is computationally simple, but slow to adapt to changes in the environment.

Predicting state transitions is useful for **planning**. Model-based RL involves explicit models for state-action-state transitions and the reward function. Model-based RL is computationally more expensive, but can adapt quickly to changes in the reward function.

Evidence for both forms of learning (Gläscher, Daw, Dayan, and O'Doherty, 2010). Arbitration between model-based and model-free RL may be based on cost-benefit tradeoff (Kool, Gershman, and Cushman, 2017)

---

## The two-step task (Daw, Gershman, Seymour, Dayan, and Dolan, 2011)

&lt;img src="img/daw_two_step_task_design.jpg" width="80%" style="display: block; margin: auto;" /&gt;

--

&lt;img src="img/daw_two_step_predictions_results.jpg" width="80%" style="display: block; margin: auto;" /&gt;


---

class: center, middle, inverse

# Opponent modelling

---

## RL in multi-agent settings

In games like Rock-Paper-Scissors, Tic-Tac-Toe, Chess, Go, the state of the environment depends also on the actions of the other player(s).

* The state-transition probabilities will change over time `\(p(S_{t+1}|S_{t}, A_t)\)` if the opponent adapts their policy
  * Model-free RL can (slowly) adapt to changing environments.
  * Model-based RL may have an advantage if the changes can be predicted 
* Even if the opponent does not change their policy, a model of the opponent is useful in **generalization** of experience to new games
  * E.g., "opponent expects me to repeat my previous action" can help provide useful actions in a variety of games

---

## Generalizing opponent models in simple games

&lt;img src="img/ownArticles/guennouni-opponent.png" width="40%" style="display: block; margin: auto auto auto 0;" /&gt;

&lt;img src="img/RPS-experiment-screen.png" width="60%" style="display: block; margin: auto;" /&gt;

--

Two types of AI opponents:
* Level-1: Expects human player to repeat last action
* Level-2: Expects human player to be a level 1 player

Also played Fire-Water-Grass and Numbers game/Shootout.

---

## Generalizing opponent models in simple games

&lt;img src="img/guennouni-exp1-early-rounds.png" width="90%" style="display: block; margin: auto;" /&gt;

---

## Generalizing opponent models in simple games

&lt;img src="img/guennouni-exp2-early-rounds.png" width="90%" style="display: block; margin: auto;" /&gt;

---

## Model-free RL does not generalize to new games

`\(Q(s, a)\)` with `\(s = \{\text{previous own action},\text{previous opponent action\}\)`

|self |opponent |  R|  P|  S|
|:----|:--------|--:|--:|--:|
|R    |R        | -1|  0|  1|
|R    |P        | -1|  0|  1|
|R    |S        | -1|  0|  1|
|P    |R        |  1| -1|  0|
|P    |P        |  1| -1|  0|
|P    |S        |  1| -1|  0|
|S    |R        |  0|  1| -1|
|S    |P        |  0|  1| -1|
|S    |S        |  0|  1| -1|

---

## Model-free RL does not generalize to new games

`\(Q(s, a)\)` with `\(s = \{\text{previous own action},\text{previous opponent action\}\)`

|self |opponent |F  |W  |G  |
|:----|:--------|:--|:--|:--|
|F    |F        |?  |?  |?  |
|F    |W        |?  |?  |?  |
|F    |G        |?  |?  |?  |
|W    |F        |?  |?  |?  |
|W    |W        |?  |?  |?  |
|W    |G        |?  |?  |?  |
|G    |F        |?  |?  |?  |
|G    |W        |?  |?  |?  |
|G    |G        |?  |?  |?  |

---

## But a Bayesian model can

The Bayesian Cognitive Hierarchy model (Guennouni &amp; Speekenbrink, 2022) expects an opponent to have a fixed "level of reasoning":

* Level-0: Adopts a fixed strategy (e.g. repeats last action)
* Level-1: Expects opponent to be a Level-1 player and best-responds to that
* Level-2: Expects opponent to be a Level-2 player and best-responds to that
* `\(\ldots\)`

but allows for errors of execution with a certain probability. From a prior over opponent types, it learns the type from the opponent's actions.

--

This is equivalent to learning a state-action-state transition function, which can be generalized to new games

---

## Early model-based, later model-free

&lt;img src="img/guennouni-exp1-HMM-posterior.png" width="90%" style="display: block; margin: auto;" /&gt;

---

## Early model-based, later model-free

&lt;img src="img/guennouni-exp2-HMM-posterior.png" width="90%" style="display: block; margin: auto;" /&gt;

---

class: center, middle, inverse

# Summary

---

## Summary

* Reinforcement learning is a theory and mathematical framework for "learning by doing"
  * It is successful in describing how humans learn, and for training machines.
* Two main classes of RL: model-free and model-based
  * Both forms seem to be used by humans, with goal-based arbitration
* Successful experiential learning and control requires balancing  *exploration* and *exploitation*
  * Evidence for uncertainty-guided exploration supports Bayesian RL
* Simple RL algorithms are unable to generalize experience to novel situations
  * Seemingly different observable states may be similar in terms of optimal actions
  * Generalization may require learning with *latent* -- not directly observable -- states (partially-observed Markov Decision Process, POMDP), or flexible causal models of the environment
  * Compositional Gaussian Processes?
* Multi-agent RL settings are complex because agents adapt their policy to each other
  * Model-free RL can adapt to a changing environment
  * Opponent models may enhance learning and generalization

---

## The RL bible

&lt;img src="img/RL-book-cover.png" width="90%" style="display: block; margin: auto;" /&gt;

---

## References

.smallish[

Daw, N. D., S. J. Gershman, B. Seymour, et al. (2011). "Model-based
influences on humans' choices and striatal prediction errors". In:
_Neuron_ 69.6, pp. 1204-1215.

Gläscher, J., N. Daw, P. Dayan, et al. (2010). "States versus rewards:
dissociable neural prediction error signals underlying model-based and
model-free reinforcement learning". In: _Neuron_ 66.4, pp. 585-595.

Kool, W., S. J. Gershman, and F. A. Cushman (2017). "Cost-benefit
arbitration between multiple reinforcement-learning systems". In:
_Psychological science_ 28.9, pp. 1321-1333.

Montague, P. R., P. Dayan, and T. J. Sejnowski (1996). "A framework for
mesencephalic dopamine systems based on predictive Hebbian learning".
In: _Journal of neuroscience_ 16.5, pp. 1936-1947.

Rescorla, R. A. and A. R. Wagner (1972). "A theory of Pavlovian
conditioning: Variations in the effectiveness of reinforcement and
non-reinforcement". In: _Classical conditioning II: Current research
and theory_, pp. 64-69.

Schulz, E., J. B. Tenenbaum, D. Duvenaud, et al. (2017). "Compositional
inductive biases in function learning". In: _Cognitive psychology_ 99,
pp. 44-79.

Skinner, B. (1938). "The behavior of organisms: An experimental
analysis".

Sutton, R. S. and A. G. Barto (1990). "Time-derivative models of
Pavlovian reinforcement". In: _Learning and computational neuroscience:
Foundations of adaptive networks_. Ed. by M. Gabriel and J. Moore.

Sutton, R. S. and A. G. Barto (2018). _Reinforcement learning: An
introduction_. 2nd edition.

Thorndike, E. L. (1911). _Animal Intelligence: Experimental Studies_.
Transaction Publishers.

Watkins, C. J. C. H. and P. Dayan (1992). "Q-learning". In: _Machine
learning_ 8.3, pp. 279-292.

]
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script src="macros.js"></script>
<script>var slideshow = remark.create({
  "highlightStyle": "github",
  "highlightLines": true,
  "countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// add `data-at-shortcutkeys` attribute to <body> to resolve conflicts with JAWS
// screen reader (see PR #262)
(function(d) {
  let res = {};
  d.querySelectorAll('.remark-help-content table tr').forEach(tr => {
    const t = tr.querySelector('td:nth-child(2)').innerText;
    tr.querySelectorAll('td:first-child .key').forEach(key => {
      const k = key.innerText;
      if (/^[a-z]$/.test(k)) res[k] = t;  // must be a single letter (key)
    });
  });
  d.body.setAttribute('data-at-shortcutkeys', JSON.stringify(res));
})(document);
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
